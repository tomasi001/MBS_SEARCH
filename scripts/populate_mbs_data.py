#!/usr/bin/env python3
"""
MBS data population script for MBS AI Assistant MVP.

This script loads MBS data from the existing database and populates
the vector database with embeddings generated by Gemini.
"""

import logging
import sys
import os
import sqlite3
from typing import List, Dict, Any

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import settings
from services.vector_service import VectorService
from services.gemini_service import GeminiService

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_mbs_data_from_db() -> List[Dict[str, Any]]:
    """Load MBS data from the existing SQLite database."""
    logger.info(f"Loading MBS data from {settings.MBS_DB_PATH}")

    documents = []

    try:
        with sqlite3.connect(settings.MBS_DB_PATH) as conn:
            cur = conn.cursor()

            # Get all items with their relations and constraints
            cur.execute(
                """
                SELECT 
                    i.item_num,
                    i.category,
                    i.group_code,
                    i.schedule_fee,
                    i.description,
                    i.derived_fee,
                    i.start_date,
                    i.end_date,
                    i.provider_type,
                    i.emsn_description
                FROM items i
                ORDER BY CAST(i.item_num AS INTEGER)
            """
            )

            items = cur.fetchall()
            logger.info(f"Found {len(items)} items in database")

            for item in items:
                item_num = item[0]
                description = item[4] or ""

                # Create description document
                if description:
                    documents.append(
                        {
                            "id": f"{item_num}_description",
                            "content": description,
                            "metadata": {
                                "item_num": item_num,
                                "chunk_type": "description",
                                "category": item[1],
                                "group_code": item[2],
                                "schedule_fee": item[3],
                                "start_date": item[6],
                                "end_date": item[7],
                                "provider_type": item[8],
                            },
                        }
                    )

                # Get relations for this item
                cur.execute(
                    """
                    SELECT relation_type, target_item_num, detail
                    FROM relations
                    WHERE item_num = ?
                """,
                    (item_num,),
                )

                relations = cur.fetchall()
                if relations:
                    relation_text = "Relations: " + "; ".join(
                        [
                            f"{rel[0]} {rel[1] or 'general'}"
                            + (f" ({rel[2]})" if rel[2] else "")
                            for rel in relations
                        ]
                    )

                    documents.append(
                        {
                            "id": f"{item_num}_relations",
                            "content": relation_text,
                            "metadata": {
                                "item_num": item_num,
                                "chunk_type": "relations",
                                "relation_count": len(relations),
                            },
                        }
                    )

                # Get constraints for this item
                cur.execute(
                    """
                    SELECT constraint_type, value
                    FROM constraints
                    WHERE item_num = ?
                """,
                    (item_num,),
                )

                constraints = cur.fetchall()
                if constraints:
                    constraint_text = "Constraints: " + "; ".join(
                        [f"{con[0]}: {con[1]}" for con in constraints]
                    )

                    documents.append(
                        {
                            "id": f"{item_num}_constraints",
                            "content": constraint_text,
                            "metadata": {
                                "item_num": item_num,
                                "chunk_type": "constraints",
                                "constraint_count": len(constraints),
                            },
                        }
                    )

            logger.info(f"Created {len(documents)} documents from MBS data")
            return documents

    except Exception as e:
        logger.error(f"Error loading MBS data: {e}")
        return []


def populate_vector_database():
    """Populate the vector database with MBS data."""
    logger.info("Starting vector database population...")

    try:
        # Initialize services
        vector_service = VectorService()
        gemini_service = GeminiService()

        # Check if database already has data
        stats = vector_service.get_collection_stats()
        if stats.get("total_documents", 0) > 0:
            logger.info(
                f"Vector database already has {stats['total_documents']} documents"
            )
            response = input("Do you want to reset and repopulate? (y/N): ")
            if response.lower() == "y":
                logger.info("Resetting vector database...")
                vector_service.reset_collection()
            else:
                logger.info("Skipping population")
                return

        # Load MBS data
        documents = load_mbs_data_from_db()

        if not documents:
            logger.error("No documents to populate")
            return

        # Add documents to vector database in batches
        batch_size = 50  # Process in smaller batches for Gemini API
        total_batches = (len(documents) + batch_size - 1) // batch_size

        logger.info(f"Processing {len(documents)} documents in {total_batches} batches")

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(documents))
            batch_documents = documents[start_idx:end_idx]

            logger.info(
                f"Processing batch {batch_idx + 1}/{total_batches} ({len(batch_documents)} documents)"
            )

            # Add batch to vector database
            success = vector_service.add_documents(batch_documents)

            if success:
                logger.info(f"Successfully added batch {batch_idx + 1}")
            else:
                logger.error(f"Failed to add batch {batch_idx + 1}")
                break

        # Get final stats
        final_stats = vector_service.get_collection_stats()
        logger.info(f"Vector database population complete!")
        logger.info(f"Total documents: {final_stats.get('total_documents', 0)}")
        logger.info(f"Unique items: {final_stats.get('unique_items_in_sample', 0)}")
        logger.info(f"Chunk types: {final_stats.get('chunk_types_in_sample', {})}")

    except Exception as e:
        logger.error(f"Error populating vector database: {e}")
        raise


if __name__ == "__main__":
    populate_vector_database()
